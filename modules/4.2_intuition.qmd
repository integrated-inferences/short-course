---
title: "Causal Questions: Population Level"
subtitle: "Intuition"
format: 
   revealjs:
      embed-resources: true
      theme: serif
      slide-level: 3
      slide-number: true
      toc-depth: 2
      show-slide-number: all
      preview-links: auto
      number-sections: true
      link-color: orange
      smaller: true
---

```{r, include = FALSE}
set.seed(1)
library(knitr)
source("assets/setup.R")
knitr::opts_chunk$set(echo = TRUE)
```


## How do we "update" our models?

* We've talked about process tracing a single case to answer a case-level query
    * Here the model is fixed
    * We use the model + case data to answer questions about the case
* We can also use data to "update" our models
    * Use data on many cases to learn about causal effects in the population
* Allows mixing methods: using data on lots of cases, we can learn about probative value of process-tracing evidence
* The core logic: we learn by **updating population-level causal beliefs toward beliefs more consistent with the data**


## Start with a DAG

```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.9\\textwidth'}
par(mar=c(1,1,1,1))
hj_dag(x = c(0, 0, 0, 2, 2, 2, 1, 1, 1),
       y = c(1, 2, 3, 1, 2, 3, 1, 2, 3),
       names = c("I", expression(theta^I), expression(lambda^I), "D", expression(theta^D), expression(lambda^D), "M", expression(theta^M), expression(lambda^M)),
       arcs = cbind( c(3, 2, 1, 5, 6, 7, 8, 9),
                     c(2, 1, 7, 4, 5, 4, 7, 8)),
       padding = .4, contraction = .15)

```


## Large-$N$ estimation of $ATE$: what happens to beliefs over parameters

* We collect data on $I$, $M$, and $D$ for a large number of cases
* We observe a strong positive correlation 
* We will think there's a positive average effect


## Large-$N$ estimation of $ATE$: what happens to beliefs over parameters

* Now, we will update on both $\lambda^I$ , $\lambda^M$ and $\lambda^D$
* An $I \rightarrow D$ effect can only happen if $I$ affects $M$ and $M$ affects $D$, in specific ways
* Two possible **combinations** of effects can generate a positive $I \rightarrow D$ effect
    * $I \rightarrow M$ is positive, $M \rightarrow D$ is positive
    * $I \rightarrow M$ is negative, $M \rightarrow D$ is negative
* So we will come to put more weight on a *joint distribution* of $\lambda^M$ and $\lambda^D$ in which there are lots of cases with one of these two combinations
* ...and less posterior weight on all other combinations of effects

## General procedure

Key insight: 

* If we *suppose* a given set of  parameter values we can figure out the likelihood of the data given those values.
* We can do this for all possible parameter values and see which ones are more in line with the data


That, with priors, is enough to update:

$$p(\lambda | D) = \frac{p(D | \lambda)p(\lambda)}{p(D)}$$
## "By hand"

Let's update manually.

### Causal inference on a grid

Consider this joint distribution with binary $X$ and binary $Y$ from [here](https://macartan.github.io/ci/ci_2025.html#/example-without-identification-2)

|     | Y = 0 | Y = 1 | 
|-----|-------|-------|
| **X = 0** | $b/2 + c/2$      |  $a/2 + d/2$     |   
| **X = 1** | $a/2 + c/2$      |  $b/2 + d/2$     |   


reminder: $a$ is share with negative effects, $b$ is share with positive effects...

### Causal inference on a grid: strategy

Say we now had (finite) data filling out this table. What posteriors should we form over $a,b,c,d$?


|     | Y = 0 | Y = 1 | 
|-----|-------|-------|
| **X = 0** | $n_{00}$      |  $n_{01}$     |   
| **X = 1** | $n_{10}$      |  $n_{11}$     |   

Lets start with a flat prior over the shares and then update over possible shares based on the data.

This time we will start with a draw of possible shares and put look for posterior weights on each drawn share.


### Causal inference on a grid: likelihood {.smaller}

$$
\Pr(n_{00}, n_{01}, n_{10}, n_{11} \mid a,b,c,d) =
f_{\text{multinomial}}\left( \alpha_{00}, \alpha_{01}, \alpha_{10}, \alpha_{11} \mid \sum n, w \right)
$$
where:

$$w = \left(\frac12(b + c),  \frac12(a+d), \frac12(a+c), \frac12(b+d)\right)$$

why multinomial?

### Causal inference on a grid: execution {.smaller}

prior draw with 10000 possibilities:

```{r}

x <- gtools::rdirichlet(10000, alpha = c(1,1,1,1)) |> as.data.frame()
names(x) <- letters[1:4]

x |> head() |> kable(digits = 3)
```
each row sums to 1; each point (row) lies on a simplex

### Causal inference on a grid: execution {.smaller}


Imagine we had data (number of units with given values of X and Y):

$n_{00} = 400, n_{01} = 100, n_{10} = 100, n_{11} = 400$

Difference in means = .6. 

Then:


```{r}
# add likelihood and calculate posterior

x <- x |> 
  rowwise() |>  # Ensures row-wise operations
  mutate(
    likelihood = dmultinom(
      c(400, 100, 100, 400),
      prob = c(b + c, a + c, a + d, b + d) / 2
    )
  ) |> 
  ungroup() |>
  mutate(posterior = likelihood / sum(likelihood))

```


### Causal inference on a grid: execution {.smaller}

```{r}
x |> 
  mutate(likelihood = formatC(likelihood, format = "e", digits = 2),
         posterior = formatC(posterior, format = "e", digits = 2)) |> 
  head() |>
  kable(digits = 2)
```

### Causal inference on a grid: inferences

```{r}
x |> summarize(a = weighted.mean(a, posterior),
               b = weighted.mean(b, posterior),
               ATE = b - a) |>
  kable(digits = 2)
```

### Causal inference on a grid: inferences

```{r, fig.cap = "Spot the ridge"}
x |> ggplot(aes(b, a, size = posterior)) + geom_point(alpha = .5) 

```


## In sum: learning from data

* For any data pattern, we gain confidence in parameter values more consistent with the data
* For single-case inference, we must bring background beliefs about population-level causal effects
* For multiple cases, we can learn about effects from the data
* Large-$N$ data can thus provide probative value for small-$N$ process-tracing
* All inference is conditional on the model





