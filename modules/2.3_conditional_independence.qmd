---
format: 
   revealjs:
      embed-resources: true
      theme: serif
      slide-level: 3
      slide-number: true
      toc-depth: 2
      show-slide-number: all
      preview-links: auto
      number-sections: true
      link-color: orange
      smaller: true
title: "Conditional independence"
author: "Macartan Humphreys"
bibliography: assets/bib.bib
---

```{r, include = FALSE}

source("assets/setup.R")

```

## Key insight linking Causal Models to Bayes

The key insight that connects a causal model to Bayesian updating is this:

* a causal model can tell you how to update on one feature given information on another

<!-- ## Key insight linking Causal Models to Bayes -->

<!-- Most simple idea.  -->

<!-- Say we have DAG: $X \rightarrow Y$ and we know: -->

<!-- * $\Pr(X = 1) = .5$ -->
<!-- * $\Pr(Y = 1 | X = 1) = .75$ -->
<!-- * $\Pr(Y = 1 | X = 0) = .25$ -->

<!-- Then we can ask a question like: what is the probability that $X=1$ if indeed $Y=1$: -->

<!-- $$\Pr(X = 1 | Y = 1) = \frac{\Pr(Y = 1 | X = 1)\Pr(X=1)}{\Pr(Y=1)} = \frac{0.75 \times 0.5}{0.5} = 0.75$$ -->


<!-- So we can *sometimes* learn about one node from others nodes, up and down a graph. -->

<!-- Let's figure out *when* we can do this. -->


## Conditional independence and graph structure {.smaller}

* What DAGs do is tell you when one variable is independent of another variable given some third variable.
* Intuitively:
  * what variables "shield off" the influence of one variable on another
  * e.g. If inequality causes revolution *via* discontent, then inequality and  revolution should be related to each other overall, but *not* related to each other *among* highly contented cases or *among* discontented cases 

### Conditional independence

Variable sets $A$ and $B$ are conditionally independent, given $C$ if for all $a$, $b$, $c$:

$$\Pr(A = a | C = c) = \Pr(A = a | B = b, C = c)$$

Informally; given $C$, knowing $B$ tells you nothing more about $A$.


### Three elemental structures for thinking about conditional independence

```{r HJ-F-2-4, echo = FALSE, fig.width = 6, fig.height = 4,  fig.align="center", out.width='70%', fig.cap = "Three elemental relations of conditional independence."}
p1 <- hj_ggdag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(1, 2),
                     c(2, 3)), labelparse = FALSE,
       title = "(1) A path of arrows pointing in the same direction", padding = .2)
p2 <- hj_ggdag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(2, 2),
                     c(1, 3)), labelparse = FALSE,
       title = "(2) A forked path", padding = .2) 
p3 <- hj_ggdag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(1, 3),
                     c(2, 2)), labelparse = FALSE,
       title = "(3) An inverted fork (collision)", padding = .2)

plot_grid(plotlist = list(p1,p2,p3),ncol=1)

```


### Conditional independence  from graphs {.smaller}

$A$ and $B$ are *conditionally independent*, given $C$ if on *every* path between $A$ and $B$:

-   there is some chain ($\bullet\rightarrow \bullet\rightarrow\bullet$ or $\bullet\leftarrow \bullet\leftarrow\bullet$) or fork ($\bullet\leftarrow \bullet\rightarrow\bullet$) with the central element in $C$ (i.e., all dependencies are blocked by $C$),

or

-   there is an inverted fork ($\bullet\rightarrow \bullet\leftarrow\bullet$) with the central element (and its descendants) *not* in $C$ (i.e., no dependencies are opened by $C$)

Notes:

-   In this case we say that $A$ and $B$ are d-separated by $C$.
-   $A$, $B$, and $C$ can all be sets
-   Note that a path can involve arrows pointing any direction $\bullet\rightarrow \bullet\rightarrow \bullet\leftarrow \bullet\rightarrow\bullet$

### Test yourself {.smaller}

```{r, echo = FALSE, fig.width = 5, fig.height = 2,  fig.align="center", out.width='.9\\textwidth'}
hj_ggdag(x = 1:4, y = c(1, 1,1,1), names = c( "A", "B", "C", "D" ),
       arcs = cbind( c(1, 3, 3),
                     c(2, 2, 4)), labelparse = FALSE,
       title = " ", padding = .4, contraction = .15) 

```

Are A and D unconditionally independent:

-   if you do not condition on anything?
-   if you condition on B?
-   if you condition on C?
-   if you condition on B and C?


## Theorem

Key insight for process tracing: 

* Informativeness of a node hinges on conditional independence
* If $K$ is *not* conditionally independent of $\theta$ then $K$ *is* informative for $\theta$.
* If $K$ *is* conditionally independent of $\theta$ then $K$ is *not* for $\theta$.

```{r, echo = FALSE}

make_model( "X-> M-> Y <- Q; K -> M") |> plot(x_coord = c(2, 2.5, 1, 1.5, 2 ), y_coord = c(2,1,2,1, 0), parse=TRUE, labels=c("K","theta","X","M","Y"))
```


### Next steps

When we turn to process tracing we will unpack the usefulness of this result

