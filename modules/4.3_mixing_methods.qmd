---
title: "Causal Questions: Population Level"
subtitle: "Mixing Methods"
format: 
   revealjs:
      embed-resources: true
      theme: serif
      slide-level: 3
      slide-number: true
      toc-depth: 2
      show-slide-number: all
      preview-links: auto
      number-sections: true
      link-color: orange
      smaller: true
---

```{r, include = FALSE}
set.seed(1)
library(here)
source(here("assets", "setup.R"))
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```


## A DAG

```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.9\\textwidth'}
par(mar=c(1,1,1,1))
hj_dag(x = c(0, 0, 0, 2, 2, 2, 1, 1, 1),
       y = c(1, 2, 3, 1, 2, 3, 2, 3, 4),
       names = c("X", expression(theta^X), expression(lambda^X), "Y", expression(theta^Y), expression(lambda^Y), "M", expression(theta^M), expression(lambda^M)),
       arcs = cbind( c(3, 2, 1, 5, 6, 7, 8, 9, 1),
                     c(2, 1, 7, 4, 5, 4, 7, 8, 4)),
       padding = .4, contraction = .15)

```

* We'll want to learn about the $\theta$'s and the $\lambda$'s
* We need to observe nodes to learn about other nodes
* We can potentially observe 3 nodes here: $X, M$, and $Y$


## A typical "quantitative" data structure


* Data on exogenous variables and a key outcome for many cases

* E.g., data on inequality ($I$) and democracy ($D$) for many cases


```{r echo=FALSE, out.width="320px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/quantdata.png")
```




## A typical "qualitative" data structure

* Data on exogenous variables and a key outcome plus elements of process for a **small** number of cases
    * Finite resources mean tradeoffs between extensive and intensive data collection

* E.g., data on inequality ($I$), mass mobilization ($M$), and democracy ($D$) for many cases


```{r echo=FALSE, out.width="320px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/qualdata.png")
```


## Mixing qualitative and quantitative

* What if we combine extensive data on many cases with intensive data on a few cases?

```{r echo=FALSE, out.width="320px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/mixeddata.png")
```

* A non-rectangular data structure

## Non-rectangular data

* A data structure that neither standard quantitative nor standard qualitative approaches can handle in a systematic way
* Not a problem for the Integrated Inferences approach
* We simply ask: 
    * Which causal effects in the population are most and least consistent with the data pattern we observe?
* That is, what distribution of causal effects in the population, for each node, are most consistent with this data pattern?
* `CausalQueries` uses information wherever it finds it

## Mixing in practice

For Bayesian approaches this mixing is not hard.

Critically though we maintain the assumption that cases for "in depth" analysis are chosen *at random*---otherwise we have to account for selection processes.


What is the probability of seeing these two cases:

1. $X=1, M = 1, Y = 1$ case 
2. $X=1, Y=1$ (no data on $M$) 

given parameters $\lambda$:

## Mixing in practice

The probability of 1 is:

$$p_{111}= \lambda^X_1 \times (\lambda^M_{01} + \lambda^M_{11}) \times (\lambda^Y_{01} +\lambda^Y_{11})$$

The probability of 2 is:

$$p_{1?1} =  \lambda^X_1\times \left((\lambda^M_{01} + \lambda^M_{11}) \times (\lambda^Y_{01} +\lambda^Y_{11}) + (\lambda^M_{10} + \lambda^M_{00}) \times (\lambda^Y_{10} +\lambda^Y_{11}) \right)$$

So the probability of this data is just:

$$p(D|\lambda) = p_{111} * p_{1?1}$$

## Mixing in practice

Insight:

If we imagine possible parameter values we can figure out the likeihood of *any* data type -- quantitative, qualitative or mixed.

That, with priors, is enough to update:

$$p(\lambda | D) = \frac{p(D | \lambda)p(\lambda)}{p(D)}= \frac{p(D | \lambda)p(\lambda)}{\int_{\lambda'}p(D|\lambda')p(\lambda')d\lambda'}$$


## Why is this useful?

### How qual can inform quant: confounding

```{r echo=FALSE, out.width="300px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/confounding.png")
```

Remember:

* Say we just observe a positive Inequality-Democratization correlation
* Could be because Inequality causes Democratization
* Could be because of confounding


### How qual can inform quant: confounding

```{r echo=FALSE, out.width="300px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/confounding.png")
```

Remember

* Observing $M$ helps
* **Process data helps address the deep problem of confounding**
* Key point: we don't need $M$ for all cases
    * Can learn from $I$ and $D$ for lots of cases and $M$ for a subset

### How qual can inform quant: observable confounder

* Another example: $M$ as the confound 

```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align = "center", out.width='.9\\textwidth'}

make_model("X->Y <-M->X") |> plot()
```


### How qual can inform quant: observable confounder

* How much can we learn from $M$ data for *some* cases?

```{r echo=FALSE, out.width="300px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/widedeep.png")
```


### How quant can inform qual: getting probative value of a clue from the data

```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.8\\textwidth'}
par(mar=c(1,1,1,1))
hj_dag(x = c(0, 0, 2, 2, 1, 1),
       y = c(1, 2, 1, 2, 1, 2),
       names = c("I", expression(theta^I), "D", expression(theta^D), "M", expression(theta^M)), 
       arcs = cbind( c(2, 1, 4, 6, 5),
                     c(1, 5, 3, 5, 3)),
       padding = .4, contraction = .15)

```

* Suppose we go to the field and we learn that mass mobilization DID occur in Malawi
    * So $M=1$

* What can we conclude?

* NOTHING YET!

### How quant can inform qual: getting probative value of a clue from the data

```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.8\\textwidth'}
par(mar=c(1,1,1,1))
hj_dag(x = c(0, 0, 2, 2, 1, 1),
       y = c(1, 2, 1, 2, 1, 2),
       names = c("I", expression(theta^I), "D", expression(theta^D), "M", expression(theta^M)), 
       arcs = cbind( c(2, 1, 4, 6, 5),
                     c(1, 5, 3, 5, 3)),
       padding = .4, contraction = .15)

```


* The pure process-tracing solution: assign our beliefs about causal effects in the population
    * E.g., beliefs that linked positive effects are more likely than linked negative effects
    * Meaning that $M=1$ in an $I=1, D=1$ case speaks in favor of $I=1$ causing $D=1$
* The mixed-methods solution: **learn** about population-level effects from large-$N$ data


### Application from the book: rule-of-law institutions and long-term growth

```{r, echo = FALSE}
model <- make_model("Mort -> RoL -> Grth <- Dist; RoL <-> Grth")
  model |> plot_model()
```
* We start with flat priors over causal types
* Gather data on all nodes for many cases

### Rule-of-law and growth: process-tracing probative value from large-$N$ data

```{r echo=FALSE, out.width="300px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/mixingappclue")
```


### Rule-of-law and growth: learning about confounding

* We allowed for confounding between rule of law and growth
    * Mortality's effects on institutions may be correlated with institutions' effects on growth
* We learn about that confounding from the data
    * Rule of law more often has a positive effect on growth where mortality has a negative effect on institutions
* Consistent with selection effects:
    * When mortality is low, settlers make institutional choices in anticipation of their growth effects


### Rule-of-law and growth: learning about confounding

What this looks like in our posteriors over nodal types:

```{r echo=FALSE, out.width="300px", fig.align="center", eval = TRUE}
knitr::include_graphics("assets/mixingappconfound")
```

A type where RoL has a positive effect on Growth is more common when Mortality has a negative effect on RoL.

### How quant can inform qual: getting probative value of a clue from the data

* Suppose we have data on $I$, $D$, and $M$ for a large number of cases
* Suppose we observe a strong positive correlation across all 3 variables
* What have we learned, under this model?
    * Positive $I \rightarrow M$ effects more likely than negative
    * Positive $M \rightarrow D$ effects more likely than negative
* So **linked positive effects** more common than linked negative effects
* Meaning that $M=1$ in an $I=1, D=1$ case speaks in favor of $I=1$ causing $D=1$
* But now we've now drawn our population-level beliefs **from the data**
* Now, we can go and process-trace
    * Did high inequality cause democratization in Malawi?
    * Observe $M$
* With conclusions grounded in case-level AND population-level evidence

