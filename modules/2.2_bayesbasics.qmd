---
format: 
   revealjs:
      embed-resources: true
      theme: serif
      slide-level: 3
      slide-number: true
      toc-depth: 2
      show-slide-number: all
      preview-links: auto
      number-sections: true
      link-color: orange
      smaller: true
title: "Bayes"
author: "Macartan Humphreys"
bibliography: assets/bib.bib
---


```{r, include = FALSE}

source("assets/setup.R")

# run <- FALSE

```


## Outline

1. Bayesian reasoning
2.  Bayesian calculations by hand

## Bayes reasoning


-   Bayesian methods are just sets of procedures to figure out how to update beliefs in light of new information.

-   We begin with a prior belief about the probability that a hypothesis is true.

-   New data then allow us to form a posterior belief about the probability of the hypothesis.


### Bayes Rule {.smaller}

Bayesian inference takes into account:

-   the consistency of the evidence with a hypothesis
-   the uniqueness of the evidence to that hypothesis
-   background knowledge about the problem.

### Illustration 1 {.smaller}

I draw a card from a deck and ask *What are the chances it is a Jack of Spades?*

-   Just 1 in 52.

Now I tell you that the card is indeed a spade. What would you guess?

-   1 in 13

What if I told you it was a heart?

-   No chance it is the Jack of Spades

What if I said it was a face card and a spade.

-   1 in 3.

### Illustration 1 {.smaller}

These answers are applications of Bayes' rule.

In each case the answer is derived by:

* assessing what is possible...
* given the new information, and then
* assessing how likely the outcome of interest is among the states that are possible. 

In each case, you calculate:

$$\text{Prob Jack of Spades | Info} = \frac{\text{Is Jack of Spades Consistent w/ Info?}}{\text{How many cards are consistent w/ Info?}} $$

### Illustration 2 **Interpreting Your Test Results** {.smaller}

You take a test to see whether you suffer from a disease that affects 1 in 100 people. The test is good in the following sense:

-   if you have the disease, then with a 99% probability it will say you have the disease
-   if you do not have it, then with a 99% probability, it will say that you do not have it

The test result says that you have the disease. What are the chances you have it?

### Illustration 2 **Interpreting Your Test Results** {.smaller}

-   It is *not* 99%. 99% is the probability of the result given the disease, but we want the probability of the disease given the result.

-   The right answer is 50%, which you can think of as the share of people that have the disease among all those that test positive. For example

-   e.g. if there were 10,000 people, then 100 would have the disease and 99 of these would test positive. But 9,900 would not have the disease and 99 of these would test positive. So the people with the disease that test positive are half of the total number testing positive.

### Illustration 2. A picture {.smaller}

```{r, out.width="70%", fig.width=12, fig.height=6.5, fig.align='center', echo = FALSE}
  p=.9    ## power of test and prior prob healthy
  s=2000  ## population size
  col5 = "red"
  col0 = "black"

  par(mfrow=c(1,2))
  	plot(c(0,1), c(0,1), axes=F, type="n", ann=F)
  	title(main ="Healthy Circles")
  	points(.175*rnorm(p*p*s)+.5, .175*rnorm(p*p*s) + .5, col = col5, pch=21, bg = col5) ##Test negative
  	points(.1*rnorm((1-p)*p*s)+.5, .1*rnorm((1-p)*p*s) + .5, col = col0, pch=21, bg = col0, cex=1.1) ## Test positive
  	box()
  
  	plot(c(0,1), c(0,1), axes=F, type="n", ann=F)
  	title(main ="Sick squares")
  	points(.1*rnorm(p*(1-p)*s)+ .5, .1*rnorm(p*(1-p)*s) + .5, col = col0, pch=22, bg = col0)
  	points(.175*rnorm((1-p)*(1-p)*s)+.5, .175*rnorm((1-p)*(1-p)*s) + .5, col = col5, pch=22, bg = col5) 
  	box()
```

What's the probability of being a circle given you are black?

### Illustration 2. More formally. {.smaller}

As an equation this might be written:

$$\text{Prob You have the Disease | Pos} = \frac{\text{How many have the disease and test pos?}}{\text{How many people test pos?}}$$

$$\frac{0.01 \times 10000 \times 0.99}{0.01 \times 10000 \times 0.99 + 0.99 \times 10000 \times 0.01} =\frac12 $$


### Two Child Problem {.smaller}

Consider last an old puzzle described in Gardner (1961).

-   Mr Smith has two children, $A$ and $B$.
-   At least one of them is a boy.
-   What are the chances they are both boys?

To be explicit about the puzzle, we will assume that the information that one child is a boy is given as a truthful answer to the question "*is at least one of the children a boy?*"

Assuming also that there is a 50% probability that a given child is a boy.

### Two Child Problem {.smaller}

As an equation:

$$\text{Prob both boys | Not both girls} = \frac{\text{Prob both boys}}{\text{Prob not both girls}} = \frac{\text{1 in 4}}{\text{3 in 4}}$$



### Bayes Rule {.smaller}

Formally, all of these equations are applications of Bayes' rule, which is a simple and powerful formula for deriving updated beliefs from new data.

The formula is:

$$
\Pr(H|\mathcal{D})= \frac{\Pr(\mathcal{D}, H)}{\Pr(\mathcal{D})}
$$

Equivalently:

$$
\Pr(H|\mathcal{D})= \frac{\Pr(\mathcal{D}|H)\Pr(H)}{\Pr(\mathcal{D})}\\
$$  

or:

$$
\Pr(H|\mathcal{D})= \frac{\Pr(\mathcal{D}|H)\Pr(H)}{\sum_{H'}\Pr(\mathcal{D}|H')\Pr(H'))}
$$
                

### Causal Problem {.smaller}

What's the probability that $Y=1$ is due to $X=1$?

\begin{eqnarray*}
\Pr(\theta^Y =  \theta^Y_{01} | X=1, Y=1)&=&\frac{\Pr(X=1, Y=1|\theta^Y_{01})\Pr(\theta^Y_{01})}{\Pr(X=1, Y=1)}\\
&=&\frac{\Pr(X=1)\Pr(\theta^Y_{01})}{\Pr(X=1)\Pr(\theta^Y_{01})+ \Pr(X=1)\Pr(\theta^Y_{11})}
\end{eqnarray*}

So:^[we assumed here that the assignment of $X$ is independent of potential outcomes: $\Pr(X=1, Y=1|\theta^Y_{01}) = \Pr(Y=1|\theta^Y_{01}, X=1)\Pr(X=1 | \theta^Y_{01})=\Pr(X=1 | \theta^Y_{01})$]

$$
\Pr(\theta^Y_{01} | X=1, Y=1)=\frac{\Pr(\theta^Y_{01})}{\Pr(\theta^Y_{01})+ \Pr(\theta^Y_{11})}
$$



### Causal Problem {.smaller}


More generally

$$\Pr(Q) = \sum_{\theta \in Q} \Pr(\theta | D)=\sum_{\theta \in Q}\frac{\Pr(D,\theta)}{\Pr(D)}=\frac{\sum_{\theta \in Q}\Pr(D,\theta)}{\Pr(D)}=\frac{\sum_{\theta \in Q \cap D}\Pr(\theta)}{\sum_{\theta\in D}\Pr(\theta)}$$

so:

$$
\frac{\text{Type consistent with the data and the query}}{\text{Type consistent with the data}}
$$

## Continuous distributions 


For continuous distributions and parameter vector $\theta$:

$$p(\theta|\mathcal{D})=\frac{p(\mathcal{D}|\theta)p(\theta)}{\int_{\theta'}p(\mathcal{D|\theta'})p(\theta')d\theta}$$

### Illustration of continuous distributions {.smaller}

Consider **the share of people in a population that voted** or **the share of people for which there is a positive treatment effect** 

* This is a quantity between 0 and 1.

* It can be represented by a continuous distribution: the beta distribution, with parameters that reflect how much data you have observed

### Beta {.smaller}

-   The Beta distribution is a distribution over the $[0,1]$ that is governed by two parameters, $\alpha$ and $\beta$.
-   In the case in which both $\alpha$ and $\beta$ are 1, the distribution is uniform -- all values are seen as equally likely.
-   As $\alpha$ rises large outcomes are seen as more likely
-   As $\beta$ rises, lower outcomes are seen as more likely.
-   If both rise proportionately the expected outcome does not change but the distribution becomes tighter.

An attractive feature is that if one has a prior Beta($\alpha$, $\beta$) over the probability of some event, and then one observes a positive case, the Bayesian posterior distribution is also a Beta with with parameters $\alpha+1, \beta$. Thus if people start with uniform priors and build up knowledge on seeing outcomes, their posterior beliefs should be Beta.

### Beta {.smaller}

Here is a set of such distributions.

```{r Betas, echo = FALSE, fig.cap="\\label{betas} Beta distributions"}
par(mfrow = c(2,3))

x <- seq(0,1,.01)
plot(x, dbeta(x, .5, .5), type = "l", main = expression(paste("Beta distribution: ", alpha, ", ", beta, " = 0.5")))
plot(x, dbeta(x, 1, 1), type = "l", main = expression(paste("Beta distribution: ", alpha, ", ", beta, " = 1")))
plot(x, dbeta(x, 1, 2), type = "l", main = expression(paste("Beta distribution: ", alpha, "= 1, ", beta, " = 2")))
plot(x, dbeta(x, 2, 1), type = "l", main = expression(paste("Beta distribution: ", alpha, "= 2, ", beta, " = 1")))
plot(x, dbeta(x, 4, 2), type = "l", main = expression(paste("Beta distribution: ", alpha, " =4, ", beta, " = 2")))
plot(x, dbeta(x, 20, 10), type = "l", main = expression(paste("Beta distribution: ", alpha, "=20, ", beta, " = 10")))

```


<!-- ## Bayes "by hand" -->

<!-- *Bayes on a Grid* -->

<!-- ### Bayes by hand  -->

<!-- * The simplest and most intuitive way to do Bayesian estimation is just to apply the formula over a grid of possible values -->
<!-- * This becomes too hard once your parameter space grows but it is worth working though the logic by hand to get a feel for Bayes -->

<!-- ### Bayes by hand  -->

<!-- * Lets say that we want to figure out the share of women in some population.  -->
<!-- * We start off with a flat prior over all possible numbers -->
<!-- * We draw a sample from the population: 100 people of which 20 are women -->
<!-- * What's our posterior? -->

<!-- ### Bayes by hand  -->

<!-- ```{r, fig.height = 3, fig.width = 8} -->
<!-- fabricate(N = 100, -->
<!--           parameters = seq(.01, .99, length = N), -->
<!--           likelihood = dbinom(20, 100, parameters), -->
<!--           posterior = likelihood/sum(likelihood)) |> -->
<!--   ggplot(aes(parameters, posterior)) + geom_line() + theme_bw() -->


<!-- ``` -->

<!-- ### Bayes by hand  -->

<!-- Now with a strongish prior on 50%: -->

<!-- ```{r, fig.height = 3, fig.width = 8} -->
<!-- fabricate(N = 100, -->
<!--           parameters = seq(.01, .99, length = N), -->
<!--           prior = dbeta(parameters, 20, 20),  -->
<!--           prior = prior/sum(prior), -->
<!--           likelihood = dbinom(20, 100, parameters), -->
<!--           posterior = likelihood*prior/sum(likelihood*prior)) |> -->
<!--   ggplot(aes(parameters, posterior)) + geom_line() + theme_bw() + -->
<!--   geom_line(aes(parameters, prior), color = "red") -->



<!-- ``` -->


<!-- ### Bayes by hand  -->

<!-- * This approach is sound, but if you are dealing with many continuous parameters, the full parameter space can get very large and so the number of calculations you do increases rapidly. -->

<!-- * Luckily other approaches have been developed. -->



